{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feedback+GAN assembly.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm6a8Ch9krFF"
      },
      "source": [
        "# Imports + Globals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNAHtAivmwwu",
        "outputId": "6fd82c4d-c47b-4d18-8dfc-9021d50f4d0d"
      },
      "source": [
        "MAX_LEN = 243\n",
        "MIN_LEN = 0\n",
        "\n",
        "TASK_TYPE = 'DNA'\n",
        "MAX_LEN_PROTEIN = MAX_LEN // 3 - 6\n",
        "MIN_LEN_PROTEIN = MIN_LEN // 3 - 6\n",
        "print(f'Protein length: {MAX_LEN_PROTEIN}')\n",
        "\n",
        "SEQ_LENGTH = MAX_LEN\n",
        "DIM = 50\n",
        "KERNEL_SIZE = 5\n",
        "BATCH_SIZE = 128\n",
        "N_CHAR = 5\n",
        "NOISE_SHAPE = 128\n",
        "\n",
        "# Check for RAM capacity purposes\n",
        "print(f'RESOURCE: {SEQ_LENGTH * DIM}')\n",
        "\n",
        "\n",
        "# SELECT PATH\n",
        "# for protein sequences\n",
        "path = '/content/gdrive/My Drive/protein_structure.csv'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Protein length: 75\n",
            "RESOURCE: 12150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUDKw7t6WE46",
        "outputId": "f881605d-1bca-4de9-823f-a6acc3b4aae9"
      },
      "source": [
        "!pip install Bio"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Bio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/db/a6665f77234574c2bf549dd4a51f58d489d7bf555e40c8f6da53a7942e68/bio-0.1.9-py3-none-any.whl (47kB)\n",
            "\r\u001b[K     |██████▉                         | 10kB 22.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 20kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 30kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 40kB 13.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from Bio) (2.23.0)\n",
            "Collecting biopython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/02/8b606c4aa92ff61b5eda71d23b499ab1de57d5e818be33f77b01a6f435a8/biopython-1.78-cp36-cp36m-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 11.1MB/s \n",
            "\u001b[?25hCollecting parasail\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/69/281553eca203ad1f76964b2f428e568e760076f49051660e6b73c846ff00/parasail-1.2.1-py2.py3-none-manylinux2010_x86_64.whl (14.1MB)\n",
            "\u001b[K     |████████████████████████████████| 14.1MB 51.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->Bio) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->Bio) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->Bio) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->Bio) (2020.11.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from biopython->Bio) (1.18.5)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (from parasail->Bio) (0.35.1)\n",
            "Installing collected packages: biopython, parasail, Bio\n",
            "Successfully installed Bio-0.1.9 biopython-1.78 parasail-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTJPtlGckcZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad34af04-91da-44c9-de29-6fe4444229a2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras.utils import to_categorical\n",
        "import seaborn as sns\n",
        "from matplotlib import rc\n",
        "import matplotlib as plt\n",
        "from keras.preprocessing import sequence\n",
        "from Bio.Seq import Seq\n",
        "from tensorflow.keras.layers import Conv1D,Input,Dense, Reshape, ReLU, Permute, Softmax\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "sns.set_context(\"paper\")\n",
        "sns.color_palette(\"cubehelix\", 8)\n",
        "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "# Using seaborn's style\n",
        "plt.style.use('seaborn')\n",
        "# With LaTex fonts\n",
        "sns.set_context(\"paper\")\n",
        "\n",
        "\n",
        "# Set the global font to be DejaVu Sans, size 10 (or any other sans-serif font of your choice!)\n",
        "rc('font',**{'family':'sans-serif','sans-serif':['DejaVu Sans'],'size':9})\n",
        "\n",
        "# Set the font used for MathJax - more on this later\n",
        "rc('mathtext',**{'default':'regular'})\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjNu_OJZrHGI"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1NLxBNmrLV1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('default')\n",
        "sns.set_context(\"paper\")\n",
        "sns.set_palette(\"husl\", 5)\n",
        "\n",
        "\n",
        "# Set the global font to be DejaVu Sans, size 10 (or any other sans-serif font of your choice!)\n",
        "rc('font',**{'family':'sans-serif','sans-serif':['DejaVu Sans'],'size':9})\n",
        "\n",
        "# Set the font used for MathJax - more on this later\n",
        "rc('mathtext',**{'default':'regular'})\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "\n",
        "def plot_history(history,name,metrics):\n",
        "\n",
        "  fig, axs = plt.subplots(len(metrics)//2,figsize=(10,15))\n",
        "  plt.subplots_adjust(hspace=0.4)\n",
        "  fig.suptitle(name)\n",
        "  # summarize history for each metric\n",
        "  for i,metric in enumerate(metrics[:4]):\n",
        "    #first half of metric dictionary is train, second half - validation\n",
        "    axs[i].plot(history.history[metric])\n",
        "    axs[i].plot(history.history['val_'+metric])\n",
        "    if metric == 'loss':\n",
        "      metric = 'binary_crossentropy'\n",
        "    axs[i].set_title('model`s ' + metric)\n",
        "    axs[i].set_ylabel(metric)\n",
        "    axs[i].set_xlabel('epoch')\n",
        "    axs[i].legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyP5pX1K0YYL"
      },
      "source": [
        "## Data misc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_49ca4Z60an6"
      },
      "source": [
        "DNA_protein_MAP = {\n",
        "            'ATA': 'I', 'ATC': 'I', 'ATT': 'I', 'ATG': 'M',\n",
        "            'ACA': 'T', 'ACC': 'T', 'ACG': 'T', 'ACT': 'T',\n",
        "            'AAC': 'N', 'AAT': 'N', 'AAA': 'K', 'AAG': 'K',\n",
        "            'AGC': 'S', 'AGT': 'S', 'AGA': 'R', 'AGG': 'R',\n",
        "            'CTA': 'L', 'CTC': 'L', 'CTG': 'L', 'CTT': 'L',\n",
        "            'CCA': 'P', 'CCC': 'P', 'CCG': 'P', 'CCT': 'P',\n",
        "            'CAC': 'H', 'CAT': 'H', 'CAA': 'Q', 'CAG': 'Q',\n",
        "            'CGA': 'R', 'CGC': 'R', 'CGG': 'R', 'CGT': 'R',\n",
        "            'GTA': 'V', 'GTC': 'V', 'GTG': 'V', 'GTT': 'V',\n",
        "            'GCA': 'A', 'GCC': 'A', 'GCG': 'A', 'GCT': 'A',\n",
        "            'GAC': 'D', 'GAT': 'D', 'GAA': 'E', 'GAG': 'E',\n",
        "            'GGA': 'G', 'GGC': 'G', 'GGG': 'G', 'GGT': 'G',\n",
        "            'TCA': 'S', 'TCC': 'S', 'TCG': 'S', 'TCT': 'S',\n",
        "            'TTC': 'F', 'TTT': 'F', 'TTA': 'L', 'TTG': 'L',\n",
        "            'TAC': 'Y', 'TAT': 'Y', 'TAA': 'P', 'TAG': 'P',\n",
        "            'TGC': 'C', 'TGT': 'C', 'TGA': 'P', 'TGG': 'W',\n",
        "        }\n",
        "\n",
        "protein_DNA_MAP = {v: k for k, v in DNA_protein_MAP.items()}\n",
        "protein_DNA_MAP['P'] = 'TAG'\n",
        "\n",
        "\n",
        "def protein_to_DNA(protein_sequences):\n",
        "    global protein_DNA_MAP\n",
        "\n",
        "    parsed = parse(protein_sequences)\n",
        "\n",
        "    DNA_sequences = []\n",
        " \n",
        "    if type(parsed[0]) in (str, np.str_):\n",
        "        DNA_merged = ''.join([a for a in parsed])\n",
        "        DNA_sequences += ['ATG'  +  DNA_merged + \"TAG\"]\n",
        "        return DNA_sequences\n",
        "\n",
        "    for seq in parsed:\n",
        "        DNA = [protein_DNA_MAP[a] for a in seq]\n",
        "        DNA_merged = ''.join([a for a in DNA])\n",
        "        DNA_sequences += ['ATG'  +  DNA_merged + \"TAG\"]\n",
        "\n",
        "    DNA_sequences = np.array(DNA_sequences).reshape(-1,1)\n",
        "    return DNA_sequences\n",
        "\n",
        "def parse(sequences):\n",
        "    if type(sequences) == str:\n",
        "        parsed = np.array([a for a in sequences])\n",
        "        return parsed\n",
        "\n",
        "    parse = lambda seq: np.array([a for a in seq])\n",
        "    parsed = pd.DataFrame(sequences).iloc[:,0].apply(parse).to_numpy()\n",
        "\n",
        "    return parsed\n",
        "\n",
        "def translate(seq): \n",
        "       \n",
        "    table = { \n",
        "        'ATA':'I', 'ATC':'I', 'ATT':'I', 'ATG':'M', \n",
        "        'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T', \n",
        "        'AAC':'N', 'AAT':'N', 'AAA':'K', 'AAG':'K', \n",
        "        'AGC':'S', 'AGT':'S', 'AGA':'R', 'AGG':'R',                  \n",
        "        'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L', \n",
        "        'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P', \n",
        "        'CAC':'H', 'CAT':'H', 'CAA':'Q', 'CAG':'Q', \n",
        "        'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R', \n",
        "        'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V', \n",
        "        'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A', \n",
        "        'GAC':'D', 'GAT':'D', 'GAA':'E', 'GAG':'E', \n",
        "        'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G', \n",
        "        'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S', \n",
        "        'TTC':'F', 'TTT':'F', 'TTA':'L', 'TTG':'L', \n",
        "        'TAC':'Y', 'TAT':'Y', 'TAA':'', 'TAG':'', \n",
        "        'TGC':'C', 'TGT':'C', 'TGA':'', 'TGG':'W', \n",
        "    } \n",
        "    protein =\"\" \n",
        "    seq = seq[0].split('P')[0]\n",
        "    for i in range(0, len(seq), 3): \n",
        "      try:\n",
        "        codon = seq[i:i + 3] \n",
        "        protein+= table[codon] \n",
        "      except:\n",
        "        protein+= \"\"\n",
        "    return protein\n",
        "\n",
        "def DNA_to_protein(sequences):\n",
        "    result = []\n",
        "    for seq in sequences:\n",
        "      result.append(translate(seq))\n",
        "    return result"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ule64wh8xSJ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "class OneHot_Seq:\n",
        "    def __init__(self, letter_type='amino acids', letters = None, max_length=MAX_LEN):\n",
        "        \"\"\"\n",
        "        :param letter_type: str 'amino acids' or 'DNA'. If a different type is used, provide custom letters.\n",
        "        :param max_length: int maximum length of a sequence. Sequences will be padded to this length.\n",
        "        \"\"\"\n",
        "\n",
        "        if letter_type == 'amino acids':\n",
        "            self.letters = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T',\n",
        "                            'W', 'Y', 'V']\n",
        "        elif letter_type == 'DNA':\n",
        "            self.letters = ['A', 'T', 'C', 'G']\n",
        "\n",
        "        else:\n",
        "            assert letters is not None\n",
        "            self.letters = letters\n",
        "\n",
        "        self.letters_dict = {f'{aa}': i + 1 for i, aa in enumerate(self.letters)}\n",
        "        self.invert_dict = {v: k for k, v in self.letters_dict.items()}\n",
        "        self.invert_dict[0] = 'P'\n",
        "\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def _parse_pad_sequences(self, sequences):\n",
        "\n",
        "        parse = lambda seq: np.array([a for a in seq])\n",
        "        parsed = pd.DataFrame(sequences).iloc[:, 0].apply(parse)\n",
        "\n",
        "        for i in range(parsed.shape[0]):\n",
        "            parsed[i] = np.vectorize(self.letters_dict.get)(parsed[i])\n",
        "\n",
        "        parsed = pad_sequences(parsed, maxlen=self.max_length, value=0, padding='post')\n",
        "\n",
        "        return parsed\n",
        "\n",
        "    def seq_to_onehot(self, sequences):\n",
        "        \"\"\"\n",
        "        Return an array of one-hot encodings from sequence strings.\n",
        "        :param sequences: ndarray of strings, shape = (N,1) where N is the number of samples\n",
        "        :return: array of onehot encoded sequences, shape = (N, max_length, amino_acids)\n",
        "        \"\"\"\n",
        "        sequences = self._parse_pad_sequences(sequences)\n",
        "        onehot = []\n",
        "\n",
        "        for seq in sequences:\n",
        "            onehot_seq = np.zeros((seq.size, len(self.letters) + 1))\n",
        "            onehot_seq[np.arange(seq.size), seq] = 1\n",
        "            onehot.append(onehot_seq)\n",
        "\n",
        "        return np.array(onehot)\n",
        "\n",
        "    def onehot_to_seq(self, sequences):\n",
        "        \"\"\"\n",
        "        Returns an array of strings from one-hot encoding.\n",
        "        :param sequences: ndarray of shape (N, max_length, amino_acids) where N is the number of samples\n",
        "        :return: array of strings of shape (N, 1)\n",
        "        \"\"\"\n",
        "        if sequences.ndim == 2:\n",
        "            sequences = np.argmax(sequences, axis=1)\n",
        "            sequences = np.vectorize(self.invert_dict.get)(sequences)\n",
        "            decoded_sequences = [''.join([aa for aa in sequences])]\n",
        "            return decoded_sequences\n",
        "\n",
        "        sequences = np.argmax(sequences, axis=2)\n",
        "        sequences = np.vectorize(self.invert_dict.get)(sequences)\n",
        "        decoded_sequences = [[''.join([aa for aa in seq])] for seq in sequences]\n",
        "\n",
        "        return decoded_sequences\n",
        "\n",
        "def get_sequences(path, split = 0.2, min_len = MIN_LEN, max_len = MAX_LEN_PROTEIN):\n",
        "    df = pd.read_csv(path)\n",
        "    input_seqs, target_seqs = df[['seq', 'sst8']][(df.len >= min_len) & (df.len <= max_len) & (~df.has_nonstd_aa)].values.T\n",
        "    seq_train, seq_test, target_train, target_test = train_test_split(input_seqs, target_seqs, test_size= split,random_state=1)\n",
        "    \n",
        "    return seq_train,seq_test,target_train,target_test\n",
        "\n",
        "def get_dataset(sequences, batch_size = BATCH_SIZE):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(sequences)\n",
        "    dataset = dataset.shuffle(sequences.shape[0], seed=0).batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "def prepare_dataset(path, split = 0.01):\n",
        "    # Load dataset for training  FBGAN\n",
        "\n",
        "    # Load protein sequences and shuffle them\n",
        "    X_train,_, _, _ = get_sequences(path, split)\n",
        "    X_train = X_train.tolist()\n",
        "    np.random.shuffle(X_train)\n",
        "\n",
        "    print(f'Number of training samples: {len(X_train)}')\n",
        "\n",
        "    # Translate to DNA encoding\n",
        "    X = protein_to_DNA(X_train)\n",
        "    #print(f'Example of translated DNA sequences: \\n {X[:3]}')\n",
        "\n",
        "    # One Hot encode into 5 categories, ATCG and P for padded positions\n",
        "    OneHot = OneHot_Seq(letter_type= 'DNA')\n",
        "    real_sequences = OneHot.seq_to_onehot(X)\n",
        "    real_sequences\n",
        "\n",
        "    #print(f'Example of OneHot encoding of DNA sequences: {real_sequences[0]}')\n",
        "\n",
        "    return real_sequences"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4lim2UVmOsu"
      },
      "source": [
        "# GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAkbVNdumSrY"
      },
      "source": [
        "## ResBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9uPJLC1mQDW"
      },
      "source": [
        "def softmax(logits):\n",
        "    shape = tf.shape(logits)\n",
        "    res = tf.nn.softmax(tf.reshape(logits, [-1,N_CHAR]))\n",
        "    return tf.reshape(res, shape)\n",
        "\n",
        "class ResidualBlock(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.relu = ReLU()\n",
        "        self.conv1d_1 = Conv1D(filters=DIM, kernel_size=KERNEL_SIZE, padding='same', strides=1, activation='relu')\n",
        "        self.conv1d_2 = Conv1D(filters=DIM, kernel_size=KERNEL_SIZE, padding='same', strides=1)\n",
        "\n",
        "    def __call__(self,X,alpha = 0.3):\n",
        "        x = self.relu(X)\n",
        "        x = self.conv1d_1(x)\n",
        "        x = self.conv1d_2(x)\n",
        "        return x + alpha*x\n",
        "\n",
        "class Generator(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        implementation of Generator\n",
        "        :param input_size: size of the sequence (input noise)\n",
        "        \"\"\"\n",
        "        super(Generator, self).__init__(name='generator')\n",
        "\n",
        "        self.model = tf.keras.models.Sequential()\n",
        "        self.model.add(Input(shape = (NOISE_SHAPE,), batch_size = BATCH_SIZE))\n",
        "        self.model.add(Dense(units = DIM*SEQ_LENGTH))\n",
        "        self.model.add(Reshape((SEQ_LENGTH, DIM)))\n",
        "\n",
        "        self.model.add(ResidualBlock())\n",
        "        self.model.add(ResidualBlock())\n",
        "        self.model.add(ResidualBlock())\n",
        "        self.model.add(ResidualBlock())\n",
        "        self.model.add(ResidualBlock())\n",
        "\n",
        "        self.model.add(Conv1D(filters = N_CHAR, kernel_size = 1))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.model(inputs)\n",
        "        x = softmax(x)\n",
        "        return x\n",
        "\n",
        "class Discriminator(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, clip = 1):\n",
        "        \"\"\"\n",
        "        implementation of Discriminator\n",
        "        :param clip: value to which you clip the gradients (or False)\n",
        "        \"\"\"\n",
        "        super(Discriminator, self).__init__(name='discriminator')\n",
        "\n",
        "        self.model = tf.keras.models.Sequential()\n",
        "        self.model.add(Input(shape = (SEQ_LENGTH,N_CHAR), batch_size = BATCH_SIZE))\n",
        "        self.model.add(Conv1D(filters = DIM, kernel_size = 1))\n",
        "\n",
        "        self.model.add(ResidualBlock())\n",
        "        self.model.add(ResidualBlock())\n",
        "        self.model.add(ResidualBlock())\n",
        "        self.model.add(ResidualBlock())\n",
        "        self.model.add(ResidualBlock())\n",
        "\n",
        "        self.model.add(Reshape((-1,DIM*SEQ_LENGTH)))\n",
        "        self.model.add(Dense(units = DIM*SEQ_LENGTH))\n",
        "        self.model.add(Dense(units = 1))\n",
        "\n",
        "    def call(self,inputs,training = False):\n",
        "        \"\"\"\n",
        "        model's forward pass\n",
        "        :param X: input of the size [batch_size, seq_length];\n",
        "        :param training: specifies the behavior of the call;\n",
        "        :return: Y: probability of each sequences being real of shape [batch_size, 1]\n",
        "        \"\"\"\n",
        "        x = self.model(inputs)\n",
        "        return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI5e0maOmWAr"
      },
      "source": [
        "## GAN class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igu4VyzumYGa"
      },
      "source": [
        "class GAN():\n",
        "\n",
        "    def __init__(self, batch_size = BATCH_SIZE, discriminator_steps = 0, lr = 0.0002, \n",
        "                 gradient_penalty_weight = 5, generator_weights_path=None, discriminator_weights_path=None):\n",
        "        self.batch_size = batch_size\n",
        "        self.G = Generator()\n",
        "        self.D = Discriminator()\n",
        "\n",
        "        self.d_steps = discriminator_steps\n",
        "        \n",
        "        self.history = {\"G_losses\": [], \"D_losses\": [], \"gradient_penalty\": [], \"sequences\": []}\n",
        "\n",
        "        self.G_optimizer = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.9)\n",
        "        self.D_optimizer = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.9)\n",
        "\n",
        "        self.gp_weight = gradient_penalty_weight\n",
        "        self.step_log = None\n",
        "\n",
        "        self.checkpoint_dir = './weights/'\n",
        "\n",
        "        if generator_weights_path:\n",
        "            self.G.load_weights(generator_weights_path)\n",
        "\n",
        "        if discriminator_weights_path:\n",
        "            self.D.load_weights(discriminator_weights_path)\n",
        "\n",
        "        \n",
        "    def generate_samples(self, number=None, decoded = False):\n",
        "        if number is None:\n",
        "            number = self.batch_size\n",
        "        z = tf.random.normal([number, NOISE_SHAPE])\n",
        "        generated = self.G(z)\n",
        "        \n",
        "        if decoded:\n",
        "            OneHot = OneHot_Seq(letter_type= TASK_TYPE)\n",
        "            generated = OneHot.onehot_to_seq(generated)\n",
        "            \n",
        "        return generated\n",
        "\n",
        "    def generator_loss(self, fake_score):\n",
        "        return -tf.math.reduce_mean(fake_score)\n",
        "\n",
        "    def discriminator_loss(self, real_score, fake_score):\n",
        "        # fake_score_mean = tf.math.reduce_mean(fake_score)\n",
        "        # real_score_mean = tf.math.reduce_mean(real_score)\n",
        "        # loss = real_score_mean - fake_score_mean\n",
        "        # return, loss, fake_score_mean, real_score_mean\n",
        "        return tf.math.reduce_mean(fake_score) - tf.math.reduce_mean(real_score)\n",
        "\n",
        "    #@tf.function\n",
        "    def gradient_penalty(self, real_samples, fake_samples):\n",
        "        alpha = tf.random.normal([self.batch_size, 1, 1], 0.0, 1.0)\n",
        "        real_samples = tf.cast(real_samples, tf.float32)\n",
        "        diff = fake_samples - real_samples\n",
        "        interpolated = real_samples + alpha * diff\n",
        "\n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            pred = self.D(interpolated, training=True)\n",
        "\n",
        "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
        "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "        \n",
        "        return gp\n",
        "\n",
        "    #@tf.function\n",
        "    def G_train_step(self):\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_samples = self.generate_samples()\n",
        "            fake_score = self.D(fake_samples, training=True)\n",
        "            G_loss = self.generator_loss(fake_score)\n",
        "\n",
        "        G_gradients = tape.gradient(G_loss, self.G.trainable_variables)\n",
        "        self.G_optimizer.apply_gradients((zip(G_gradients, self.G.trainable_variables)))\n",
        "\n",
        "        return G_loss\n",
        "\n",
        "    #@tf.function\n",
        "    def D_train_step(self, real_samples):\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_samples = self.generate_samples()\n",
        "            real_score = self.D(real_samples, training=True)\n",
        "            fake_score = self.D(fake_samples, training=True)\n",
        "\n",
        "            D_loss = self.discriminator_loss(real_score, fake_score)\n",
        "            GP = self.gradient_penalty(real_samples, fake_samples) * self.gp_weight\n",
        "            D_loss = D_loss + GP\n",
        "\n",
        "        D_gradients = tape.gradient(D_loss, self.D.trainable_variables)\n",
        "        self.D_optimizer.apply_gradients((zip(D_gradients, self.D.trainable_variables)))\n",
        "\n",
        "        return D_loss, GP\n",
        "\n",
        "    def create_dataset(self, inputs):\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
        "        dataset = dataset.shuffle(inputs.shape[0], seed=0).batch(self.batch_size, drop_remainder = True)\n",
        "        return dataset\n",
        "\n",
        "\n",
        "    def train(self, inputs, epochs, step_log = 50, save_per_epochs = None):\n",
        "        n_steps = len(self.create_dataset(inputs)) * epochs\n",
        "        step = 0\n",
        "        self.step_log = step_log\n",
        "        \n",
        "        if save_per_epochs is None:\n",
        "            save_per_epochs = epochs - 1\n",
        "        \n",
        "        # Pre-train discriminator \n",
        "        print('Pretraining discriminator...')\n",
        "        for step in range(self.d_steps):\n",
        "            dataset = self.create_dataset(inputs)\n",
        "            \n",
        "            for sample_batch in dataset:\n",
        "                self.D_train_step(sample_batch)\n",
        "\n",
        "        # Train discriminator and generator\n",
        "        for epoch in range(epochs):\n",
        "            dataset = self.create_dataset(inputs)\n",
        "\n",
        "            print(f\"Epoch {epoch}/{epochs}:\")\n",
        "\n",
        "            for sample_batch in dataset:\n",
        "                G_loss = self.G_train_step()\n",
        "                D_loss, GP = self.D_train_step(sample_batch)\n",
        "                \n",
        "                if step %  self.step_log == 0:\n",
        "                    example_sequence = self.get_highest_scoring()\n",
        "                    self.history[\"G_losses\"].append(G_loss.numpy())\n",
        "                    self.history[\"D_losses\"].append(D_loss.numpy())\n",
        "                    self.history['gradient_penalty'].append(GP.numpy())\n",
        "                    self.history['sequences'].append(example_sequence)\n",
        "                    print(f'\\t Step {step}/{n_steps} \\t Generator: {G_loss.numpy()} \\t Discriminator: {D_loss.numpy()} \\t Sequence: {example_sequence}')\n",
        "                step += 1\n",
        "            \n",
        "            if epoch % save_per_epochs == 0:\n",
        "                self.G.save_weights(os.path.join(self.checkpoint_dir, f'E{epoch}_Generator'))\n",
        "                self.D.save_weights(os.path.join(self.checkpoint_dir, f'E{epoch}_Discriminator'))\n",
        "                \n",
        "\n",
        "    def get_highest_scoring(self, num_to_generate = BATCH_SIZE, num_to_return = 1, decoded = True):\n",
        "        fake_samples = self.generate_samples(num_to_generate)\n",
        "        fake_scores = self.D(fake_samples)\n",
        "        best_indx = np.argmax(fake_scores)\n",
        "        best_seq = fake_samples[best_indx].numpy()\n",
        "\n",
        "        if decoded:\n",
        "            OneHot = OneHot_Seq(letter_type=TASK_TYPE)\n",
        "            best_seq = OneHot.onehot_to_seq(best_seq)\n",
        "\n",
        "        return best_seq\n",
        "\n",
        "    def plot_history(self):\n",
        "        D_losses = np.array(self.history['D_losses'])\n",
        "        G_losses = np.array(self.history['G_losses'])\n",
        "\n",
        "        plt.plot(np.arange(D_losses.shape[0]), D_losses, label='Discriminator loss')\n",
        "        plt.plot(np.arange(G_losses.shape[0]), G_losses, label='Generator loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel(f'Steps (x{self.step_log})')\n",
        "        plt.legend()\n",
        "        \n",
        "        plt.show()\n",
        "\n",
        "    def show_sequences_history(self):\n",
        "        sequences_history = self.history['sequences']\n",
        "        print('History of top scoring generated sequences... \\n')\n",
        "        for i in range(len(sequences_history)):\n",
        "            print(f'Step {i*self.step_log}: \\t {sequences_history[i][0]}')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7NmerVUm60j"
      },
      "source": [
        "# Feedback Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Cddm9FUnMjo"
      },
      "source": [
        "## Train from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQFGs_-onZ4q"
      },
      "source": [
        "### Get & Transform data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjFulGZ1nf61"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "global n_words, MAX_LEN\n",
        "MAX_LEN = 128 #length of max sequence we want to consider for training\n",
        "n_tags = 8 #number of classes in 8-state prediction\n",
        "\n",
        "\n",
        "def triplets(sequences):\n",
        "    \"\"\"\n",
        "    Apply sliding window of length 3 to each sequence in the input list\n",
        "    :param sequences: list of sequences\n",
        "    :return: numpy array of triplets for each sequence\n",
        "    Usage: Split protein sequence into triplets of aminoacids\n",
        "    \"\"\"\n",
        "    return np.array([[aminoacids[i:i+3] for i in range(len(aminoacids))] for aminoacids in sequences])\n",
        "\n",
        "def transform_sequence(seqs, tokenizer_encoder = None):\n",
        "  # transforms sequences for input into feedback net, tokenizes + adds padding\n",
        "  # if there is no given tokenizer_encoder -> initialize one and fit it on a given sequence\n",
        "  # o.w. just transform the sequence with given tokenizer\n",
        "  # returns transformed sequences + tokenizer that was fit on the input dataset\n",
        "  if not tokenizer_encoder:\n",
        "    tokenizer_encoder = Tokenizer()\n",
        "    input_grams = triplets(seqs)\n",
        "    tokenizer_encoder.fit_on_texts(input_grams)\n",
        "  transformed = tokenizer_encoder.texts_to_sequences(input_grams)\n",
        "  transformed = sequence.pad_sequences(transformed, maxlen=MAX_LEN, padding='post')\n",
        "  return transformed,tokenizer_encoder\n",
        "\n",
        "\n",
        "def get_data_for_feedback(path='/content/gdrive/My Drive/protein_structure.csv', MAX_LEN = 128):\n",
        "  df = pd.read_csv('/content/gdrive/My Drive/protein_structure.csv')\n",
        "  input_seqs, target_seqs = df[['seq', 'sst8']][(df.len <= MAX_LEN) & (~df.has_nonstd_aa)].values.T\n",
        "\n",
        "  # Transform features\n",
        "  input_data,tokenizer = transform_sequence(input_seqs)\n",
        "  #Transform targets\n",
        "  mlb = MultiLabelBinarizer()\n",
        "  target_data = mlb.fit_transform(target_seqs)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(input_data, target_data, test_size=.3, random_state=1)\n",
        "  return X_train,X_test,y_train,y_test, tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqSiGBYrngTT"
      },
      "source": [
        "### Declare & train the net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv7elsmwm_jg"
      },
      "source": [
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional, LayerNormalization\n",
        "\n",
        "global n_words,save_feedback\n",
        "X_train, X_test, y_train, y_test, tokenizer = get_data_for_feedback()\n",
        "n_words = len(tokenizer.word_index) + 1\n",
        "save_feedback = \"/content/gdrive/My Drive/saved_models/multilabel_feedback\" #path where we want to save weights\n",
        "\n",
        "class Feedback():\n",
        "  def __init__(self):\n",
        "    input = Input(shape=(MAX_LEN,))\n",
        "    x = Embedding(input_dim=n_words, output_dim=128, input_length=MAX_LEN)(input)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Bidirectional(LSTM(units=128, return_sequences=True,use_bias=True))(x)\n",
        "    x = Bidirectional(LSTM(units=128, return_sequences=True,use_bias=True))(x)\n",
        "    x = Bidirectional(LSTM(units=128,use_bias=True))(x)\n",
        "    y = Dense(n_tags, activation=\"sigmoid\")(x)\n",
        "    self.model = Model(input, y)\n",
        "  \n",
        "  def train(self,OPTIM=\"rmsprop\", LOSS='binary_crossentropy', BATCH_SIZE =128, EPOCHS = 5):\n",
        "    self.model.compile(optimizer=OPTIM, loss=LOSS, metrics=[tf.keras.metrics.Precision(), \n",
        "                                                                            tf.keras.metrics.Recall(),\n",
        "                                                                            tf.keras.metrics.Hinge()])\n",
        "    history = self.model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "                              validation_data=(X_test, y_test), verbose=1)\n",
        "    self.model.save(save_feedback)\n",
        "    return history"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "6ieosogIsdjZ",
        "outputId": "07cac3dc-56b0-49f1-e098-0b86eb65f813"
      },
      "source": [
        "#model = Feedback()\n",
        "#history = model.train()\n",
        "#plot_history(history, \"Feedback Net for Multilabel Classification of Sequence\",list(history.history.keys()))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8fc4ba764ac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Feedback Net for Multilabel Classification of Sequence\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-fc89b1a83374>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, OPTIM, LOSS, BATCH_SIZE, EPOCHS)\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                                             tf.keras.metrics.Hinge()])\n\u001b[1;32m     24\u001b[0m     history = self.model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n\u001b[0;32m---> 25\u001b[0;31m                               validation_data=(X_test, y_test), verbose=1)\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_feedback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kms1GxFQnP3e"
      },
      "source": [
        "## Load weights for feedback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIzQ0v22wAHu"
      },
      "source": [
        "save_feedback = \"/content/gdrive/My Drive/saved_models/multilabel_feedback\" #path where weights are saved\n",
        "\n",
        "Feedback = tf.keras.models.load_model(save_feedback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t8juz-bmD_V"
      },
      "source": [
        "# Feedback GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaUco7jJmGJE"
      },
      "source": [
        "class GAN_FBNet():\n",
        "\n",
        "    def __init__(self, generator_path=None, discriminator_path=None,\n",
        "                 fbnet_path=\"/content/gdrive/My Drive/saved_models/multilabel_feedback\", features=['C','H','E']):\n",
        "        self.GAN = GAN(generator_weights_path=generator_path, discriminator_weights_path=discriminator_path)\n",
        "        self.FBNet = tf.keras.models.load_model(fbnet_path)\n",
        "        _,_,_,_,self.tokenizer = get_data_for_feedback()\n",
        "        self.desired_features = features\n",
        "        self.data = None\n",
        "        self.checkpoint_dir = './'\n",
        "\n",
        "    def get_scores(self, inputs):\n",
        "        # convert the DNA sequences to protein sequences\n",
        "        protein_sequence = DNA_to_protein(inputs)\n",
        "        input_grams = triplets(protein_sequence)\n",
        "        transformed = self.tokenizer.texts_to_sequences(input_grams)\n",
        "        transformed = sequence.pad_sequences(transformed, maxlen=MAX_LEN, padding='post')\n",
        "        # use FBNet to grade the sequences\n",
        "        scores = self.FBNet.predict(transformed)\n",
        "        return scores\n",
        "\n",
        "    def add_samples(self, generated, scores, score_threshold=0.75, replace=False):\n",
        "        label_order = np.array(['B','C','E','G','H','I','S','T']) #order of labels as output by Multilabel binarizer - don't change!\n",
        "        best_index = scores > score_threshold\n",
        "        best_samples = []\n",
        "        for i in range(len(best_index)):\n",
        "          passed_threshold = set(label_order[best_index[i]])\n",
        "          if set(self.desired_features).issubset(passed_threshold):\n",
        "            best_samples.append(generated[i])\n",
        "\n",
        "        if replace:\n",
        "            pass\n",
        "        else:\n",
        "            self.data = np.concatenate((self.data, np.array(best_samples)), axis=0)\n",
        "        return best_samples\n",
        "\n",
        "    def train(self, inputs, epochs, step_log=50, steps_per_epoch = 100, batch_size = BATCH_SIZE):\n",
        "        self.data = inputs\n",
        "        self.batch_size = BATCH_SIZE\n",
        "\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            dataset = self.create_dataset(self.data)\n",
        "\n",
        "            print(f'Epoch {epoch} / {epochs}')\n",
        "            \n",
        "            step = 0\n",
        "\n",
        "            for sample_batch in dataset:\n",
        "                G_loss = self.GAN.G_train_step()\n",
        "                D_loss, GP = self.GAN.D_train_step(sample_batch)\n",
        "\n",
        "                generated = self.GAN.generate_samples(number=BATCH_SIZE, decoded=False)\n",
        "                OneHot = OneHot_Seq(letter_type= TASK_TYPE)\n",
        "                decoded_generated = OneHot.onehot_to_seq(generated)\n",
        "                scores = self.get_scores(decoded_generated)\n",
        "                generated = tf.cast(generated, tf.float32)\n",
        "                best_samples = self.add_samples(generated, scores)\n",
        "                    \n",
        "                self.data = np.concatenate((self.data, generated), axis=0)\n",
        "\n",
        "                if step % step_log == 0:\n",
        "                        print(f'\\tStep {step}:   Generator: {G_loss.numpy()}   Discriminator: {D_loss.numpy()}   Samples: {len(self.data)}')\n",
        "                    \n",
        "                if step == 100:\n",
        "                    break\n",
        "\n",
        "                step += 1\n",
        "            \n",
        "            percent_fake = int((len(self.data) - len(inputs)) / len(self.data)*100)\n",
        "            print(f'\\tPercent of the fake samples in the discriminator: {percent_fake}%.')\n",
        "            \n",
        "                \n",
        "                \n",
        "    def create_dataset(self, inputs):\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
        "        dataset = dataset.shuffle(inputs.shape[0], seed=0).batch(self.batch_size, drop_remainder=True)\n",
        "        return dataset"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnUMFn968oNU"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HeVDNJ78rG7",
        "outputId": "f2545c14-fcfd-490e-f96b-feb387d488a4"
      },
      "source": [
        "real_sequences = prepare_dataset(path)\n",
        "path_G = '/content/gdrive/My Drive/CS496 final project/weights/weights_generator_243'\n",
        "path_D = '/content/gdrive/My Drive/CS496 final project/weights/weights_discriminator_243'\n",
        "\n",
        "ganfb = GAN_FBNet(path_G,path_D)\n",
        "#samples = ganfb.GAN.generate_samples(number = 3, decoded=True)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 33633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVvyREmqnNKn",
        "outputId": "352e3f0c-7fc3-41a6-99ba-11a78558214f"
      },
      "source": [
        "ganfb.train(real_sequences, epochs = 2, step_log = 5)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 / 2\n",
            "WARNING:tensorflow:Layer discriminator is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "\tStep 0:   Generator: 4.670596599578857   Discriminator: -1.3110212087631226   Samples: 33781\n",
            "\tStep 5:   Generator: 3.281898021697998   Discriminator: -1.1102633476257324   Samples: 34542\n",
            "\tStep 10:   Generator: -0.34795498847961426   Discriminator: -1.1996839046478271   Samples: 35288\n",
            "\tStep 15:   Generator: 9.837810516357422   Discriminator: -0.7585580945014954   Samples: 36054\n",
            "\tStep 20:   Generator: -2.8153152465820312   Discriminator: -1.2810139656066895   Samples: 36805\n",
            "\tStep 25:   Generator: 1.6536716222763062   Discriminator: -1.0481069087982178   Samples: 37548\n",
            "\tStep 30:   Generator: 11.568453788757324   Discriminator: -0.35034650564193726   Samples: 38318\n",
            "\tStep 35:   Generator: -4.88857364654541   Discriminator: -1.0983119010925293   Samples: 39110\n",
            "\tStep 40:   Generator: -3.4093503952026367   Discriminator: -0.6188688278198242   Samples: 39855\n",
            "\tStep 45:   Generator: 8.26821231842041   Discriminator: -1.5836738348007202   Samples: 40605\n",
            "\tStep 50:   Generator: -0.6250389814376831   Discriminator: -1.5211044549942017   Samples: 41396\n",
            "\tStep 55:   Generator: 9.369855880737305   Discriminator: -1.8532637357711792   Samples: 42154\n",
            "\tStep 60:   Generator: 2.1442573070526123   Discriminator: 0.48673006892204285   Samples: 42910\n",
            "\tStep 65:   Generator: -5.320676326751709   Discriminator: -1.13015878200531   Samples: 43629\n",
            "\tStep 70:   Generator: 3.4498043060302734   Discriminator: -0.533722460269928   Samples: 44376\n",
            "\tStep 75:   Generator: 6.761690139770508   Discriminator: -1.8443069458007812   Samples: 45156\n",
            "\tStep 80:   Generator: 1.770576000213623   Discriminator: -1.0566691160202026   Samples: 45940\n",
            "\tStep 85:   Generator: -1.8410732746124268   Discriminator: -0.7408340573310852   Samples: 46692\n",
            "\tStep 90:   Generator: 2.9842453002929688   Discriminator: -0.9801138043403625   Samples: 47451\n",
            "\tStep 95:   Generator: 1.2991502285003662   Discriminator: -0.6892067193984985   Samples: 48235\n",
            "\tStep 100:   Generator: -4.107796669006348   Discriminator: -1.4090389013290405   Samples: 48997\n",
            "\tPercent of the fake samples in the discriminator: 31%.\n",
            "Epoch 1 / 2\n",
            "\tStep 0:   Generator: -2.077761650085449   Discriminator: -1.1144187450408936   Samples: 49155\n",
            "\tStep 5:   Generator: 8.817825317382812   Discriminator: -0.406637966632843   Samples: 49938\n",
            "\tStep 10:   Generator: 8.31092643737793   Discriminator: -0.6125542521476746   Samples: 50688\n",
            "\tStep 15:   Generator: -5.981394290924072   Discriminator: -0.7876596450805664   Samples: 51440\n",
            "\tStep 20:   Generator: 22.35377311706543   Discriminator: -1.3230030536651611   Samples: 52193\n",
            "\tStep 25:   Generator: 15.385894775390625   Discriminator: 0.43559640645980835   Samples: 52962\n",
            "\tStep 30:   Generator: -15.634513854980469   Discriminator: -0.17476516962051392   Samples: 53707\n",
            "\tStep 35:   Generator: 0.43650341033935547   Discriminator: -0.057412371039390564   Samples: 54445\n",
            "\tStep 40:   Generator: 31.370018005371094   Discriminator: 0.9428531527519226   Samples: 55179\n",
            "\tStep 45:   Generator: 0.3137025535106659   Discriminator: -0.798953115940094   Samples: 55894\n",
            "\tStep 50:   Generator: -1.7463645935058594   Discriminator: -0.2372664213180542   Samples: 56674\n",
            "\tStep 55:   Generator: 9.950247764587402   Discriminator: -0.7205983996391296   Samples: 57454\n",
            "\tStep 60:   Generator: 1.2080750465393066   Discriminator: -0.5144335031509399   Samples: 58235\n",
            "\tStep 65:   Generator: 3.8055410385131836   Discriminator: -0.802546501159668   Samples: 58982\n",
            "\tStep 70:   Generator: -5.157523155212402   Discriminator: -0.12725830078125   Samples: 59747\n",
            "\tStep 75:   Generator: 1.8566888570785522   Discriminator: -0.8362358808517456   Samples: 60562\n",
            "\tStep 80:   Generator: -3.7469029426574707   Discriminator: -0.6443027853965759   Samples: 61402\n",
            "\tStep 85:   Generator: -1.2090578079223633   Discriminator: -0.7193124890327454   Samples: 62125\n",
            "\tStep 90:   Generator: 1.316109538078308   Discriminator: -0.4015547037124634   Samples: 62895\n",
            "\tStep 95:   Generator: 7.670952796936035   Discriminator: -0.4744977653026581   Samples: 63665\n",
            "\tStep 100:   Generator: -4.415731430053711   Discriminator: -0.604096531867981   Samples: 64394\n",
            "\tPercent of the fake samples in the discriminator: 47%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8kF5gb8GAZE",
        "outputId": "d36a0bca-5806-49f8-f4a7-85ff46fbfb53"
      },
      "source": [
        "samples = ganfb.GAN.generate_samples(number = 3, decoded=True)\n",
        "ganfb.get_scores(samples)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.5763797e-02, 9.9999893e-01, 2.5260700e-02, 5.3206003e-01,\n",
              "        7.2146226e-03, 2.5288837e-03, 8.7424928e-01, 5.8686954e-01],\n",
              "       [2.3276709e-02, 9.9999762e-01, 8.8287042e-03, 1.2256408e-02,\n",
              "        9.4898464e-04, 4.6681748e-03, 8.5840762e-01, 1.7464902e-01],\n",
              "       [5.9732851e-02, 9.9999893e-01, 9.1261488e-01, 1.4057499e-02,\n",
              "        9.7791183e-01, 9.4469020e-04, 8.9126968e-01, 7.4861902e-01]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}